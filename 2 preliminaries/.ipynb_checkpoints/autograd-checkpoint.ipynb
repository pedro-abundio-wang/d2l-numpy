{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Automatic Differentiation\n",
    "\n",
    ":label:`chapter_autograd`\n",
    "\n",
    "\n",
    "In machine learning, we *train* models, updating them successively \n",
    "so that they get better and better as they see more and more data. \n",
    "Usually, *getting better* means minimizing a *loss function*, \n",
    "a score that answers the question \"how *bad* is our model?\"\n",
    "This question is more subtle than it appears.\n",
    "Ultimately, what we really care about \n",
    "is producing a model that performs well on data \n",
    "that we have never seen before.\n",
    "But we can only fit the model to data that we can actually see.\n",
    "Thus we can decompose the task of fitting models into two key concerns:\n",
    "*optimization* the process of fitting our models to observed data\n",
    "and *generalization* the mathematical principles and practitioners wisdom\n",
    "that guide as to how to produce models whose validity extends \n",
    "beyond the exact set of datapoints used to train it. \n",
    "\n",
    "This section addresses the calculation of derivatives,\n",
    "a crucial step in nearly all deep learning optimization algorithms.\n",
    "With neural networks, we typically choose loss functions \n",
    "that are differentiable with respect to our model's parameters.\n",
    "Put simply, this means that for each parameter, \n",
    "we can determine how rapidly the loss would increase or decrease,\n",
    "were we to *increase* or *decrease* that parameter\n",
    "by an infinitessimally small amount.\n",
    "While the calculations for taking these derivatives are straightforward,\n",
    "requiring only some basic calculus, \n",
    "for complex models, working out the updates by hand\n",
    "can be a pain (and often error-prone).\n",
    "\n",
    "The autograd package expedites this work \n",
    "by automatically calculating derivatives. \n",
    "And while many other libraries require \n",
    "that we compile a symbolic graph to take automatic derivatives, \n",
    "`autograd` allows us to take derivatives \n",
    "while writing  ordinary imperative code. \n",
    "Every time we pass data through our model, \n",
    "`autograd` builds a graph on the fly, \n",
    "tracking which data combined through \n",
    "which operations to produce the output. \n",
    "This graph enables `autograd` \n",
    "to subsequently backpropagate gradients on command. \n",
    "Here, *backpropagate* simply means to trace through the compute graph, \n",
    "filling in the partial derivatives with respect to each parameter. \n",
    "If you are unfamiliar with some of the math, \n",
    "e.g., gradients, please refer to :numref:`chapter_math`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "attributes": {
     "classes": [],
     "id": "",
     "n": "1"
    }
   },
   "outputs": [],
   "source": [
    "from mxnet import autograd, np, npx\n",
    "npx.set_np()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## A Simple Example\n",
    "\n",
    "As a toy example, say that we are interested \n",
    "in differentiating the mapping \n",
    "$y = 2\\mathbf{x}^{\\top}\\mathbf{x}$ \n",
    "with respect to the column vector $\\mathbf{x}$. \n",
    "To start, let's create the variable `x` and assign it an initial value."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "attributes": {
     "classes": [],
     "id": "",
     "n": "2"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0., 1., 2., 3.])"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = np.arange(4)\n",
    "x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that before we even calculate the gradient \n",
    "of ``y`` with respect to ``x``, \n",
    "we will need a place to store it. \n",
    "It's important that we do not allocate new memory\n",
    "every time we take a derivative with respect to a parameter\n",
    "because we will often update the same parameters \n",
    "thousands or millions of times \n",
    "and could quickly run out of memory.\n",
    "\n",
    "Note also that a gradient with respect to a vector $x$ \n",
    "is itself vector-valued and has the same shape as $x$.\n",
    "Thus it is intuitive that in code, \n",
    "we will access a gradient taken with respect to `x` \n",
    "as an attribute the `ndarray` `x` itself.\n",
    "We allocate memory for an `ndarray`'s gradient\n",
    "by invoking its ``attach_grad()`` method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "attributes": {
     "classes": [],
     "id": "",
     "n": "3"
    }
   },
   "outputs": [],
   "source": [
    "x.attach_grad()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After we calculate a gradient taken with respect to `x`, \n",
    "we will be able to access it via the `.grad` attribute. \n",
    "As a safe default, `x.grad` initializes as an array containing all zeros.\n",
    "That's sensible because our most common use case \n",
    "for taking gradient in deep learning is to subsequently \n",
    "update parameters by adding (or subtracting) the gradient\n",
    "to maximize (or minimize) the differentiated function.\n",
    "By initializing the gradient to $\\mathbf{0}$,\n",
    "we ensure that any update accidentally exectuted \n",
    "before a gradient has actually been calculated\n",
    "will not alter the variable's value."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0., 0., 0., 0.])"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x.grad"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's calculate ``y``. \n",
    "Because we wish to subsequently calculate gradients \n",
    "we want MXNet to generate a computation graph on the fly. \n",
    "We could imagine that MXNet would be turning on a recording device \n",
    "to capture the exact path by which each variable is generated.\n",
    "\n",
    "Note that building the computation graph \n",
    "requires a nontrivial amount of computation. \n",
    "So MXNet will only build the graph when explicitly told to do so. \n",
    "We can invoke this behavior by placing our code \n",
    "inside a ``with autograd.record():`` block."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "attributes": {
     "classes": [],
     "id": "",
     "n": "4"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(28.)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "with autograd.record():\n",
    "    y = 2.0 * np.dot(x, x)\n",
    "y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since `x` is an `ndarray` of length 4, \n",
    "`np.dot` will perform an inner product of `x` and `x`,\n",
    "yielding the scalar output that we assign to `y`. \n",
    "Next, we can automatically calculate the gradient of `y`\n",
    "with respect to each component of `x` \n",
    "by calling `y`'s `backward` function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "attributes": {
     "classes": [],
     "id": "",
     "n": "5"
    }
   },
   "outputs": [],
   "source": [
    "y.backward()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If we recheck the value of `x.grad`, we will find its contents overwritten by the newly calculated gradient."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 0.,  4.,  8., 12.])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x.grad"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The gradient of the function $y = 2\\mathbf{x}^{\\top}\\mathbf{x}$ \n",
    "with respect to $\\mathbf{x}$ should be $4\\mathbf{x}$. \n",
    "Let's quickly verify that our desired gradient was calculated correctly.\n",
    "If the two `ndarray`s are indeed the same, \n",
    "then their difference should consist of all zeros."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "attributes": {
     "classes": [],
     "id": "",
     "n": "6"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0., 0., 0., 0.])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x.grad - 4 * x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If we subsequently compute the gradient of another variable\n",
    "whose value was calculated as a function of `x`, \n",
    "the contents of `x.grad` will be overwritten."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1., 1., 1., 1.])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "with autograd.record():\n",
    "    y = x.sum()\n",
    "y.backward()\n",
    "x.grad"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Backward for Non-scalar Variable\n",
    "\n",
    "Technically, when `y` is not a scalar, \n",
    "the most natural interpretation of the gradient of `y` (a vector of length $m$)\n",
    "with respect to `x` (a vector of length $n$) is the Jacobian (an $m\\times n$ matrix).\n",
    "For higher-order and higher-dimensional $y$ and $x$, \n",
    "the Jacobian could be a gnarly high order tensor \n",
    "and complex to compute (refer to :numref:`chapter_math`). \n",
    "\n",
    "However, while these more exotic objects do show up \n",
    "in advanced machine learning (including in deep learning),\n",
    "more often when we are calling backward on a vector,\n",
    "we are trying to calculate the derivatives of the loss functions\n",
    "for each constitutent of a *batch* of training examples.\n",
    "Here, our intent is not to calculate the Jacobian\n",
    "but rather the sum of the partial derivatives \n",
    "computed individuall for each example in the batch.\n",
    "\n",
    "Thus when we invoke backwards on a vector-valued variable,\n",
    "MXNet assumes that we want the sum of the gradients.\n",
    "In short, MXNet, will create a new scalar variable \n",
    "by summing the elements in `y`,\n",
    "and compute the gradient of that variable with respect to `x`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0., 0., 0., 0.])"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "with autograd.record():  # y is a vector\n",
    "    y = x * x\n",
    "y.backward()\n",
    "\n",
    "u = x.copy()\n",
    "u.attach_grad()\n",
    "with autograd.record():  # v is scalar\n",
    "    v = (u * u).sum()\n",
    "v.backward()\n",
    "\n",
    "x.grad - u.grad"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Advanced Autograd\n",
    "\n",
    "Already you know enough to employ `autograd` and `ndarray` \n",
    "successfully to develop many practical models. \n",
    "While the rest of this section is not necessary just yet,\n",
    "we touch on a few advanced topics for completeness. \n",
    "\n",
    "### Detach Computations\n",
    "\n",
    "Sometimes, we wish to  move some calculations \n",
    "outside of the recorded computation graph. \n",
    "For example, say that `y` was calculated as a function of `x`.\n",
    "And that subsequently `z` was calcatated a function of both `y` and `x`. \n",
    "Now, imagine that we wanted to calculate \n",
    "the gradient of `z` with respect to `x`,\n",
    "but wanted for some reason to treat `y` as a constant,\n",
    "and only take into account the role \n",
    "that `x` played after `y` was calculated.\n",
    "\n",
    "Here, we can call `u = y.detach()` to return a new variable \n",
    "that has the same values as `y` but discards any information\n",
    "about how `u` was computed. \n",
    "In other words, the gradient will not flow backwards through `u` to `x`. \n",
    "This will provide the same functionality as if we had\n",
    "calculated `u` as a function of `x` outside of the `autograd.record` scope, \n",
    "yielding a `u` that will be treated as a constant in any called to `backward`. \n",
    "The following backward computes $\\partial (u \\odot x)/\\partial x$ \n",
    "instead of $\\partial (x \\odot x \\odot x) /\\partial x$,\n",
    "where $\\odot$ stands for element-wise multiplication."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0., 0., 0., 0.])"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "with autograd.record():\n",
    "    y = x * x\n",
    "    u = y.detach()\n",
    "    z = u * x\n",
    "z.backward()\n",
    "x.grad - u"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since the computation of $y$ was recorded, \n",
    "we can subsequently call `y.backward()` to get $\\partial y/\\partial x = 2x$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0., 0., 0., 0.])"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y.backward()\n",
    "x.grad - 2*x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Attach Gradients to Internal Variables\n",
    "\n",
    "Attaching gradients to a variable `x` implicitly calls `x=x.detach()`. \n",
    "If `x` is computed based on other variables, \n",
    "this part of computation will not be used in the backward function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1. 1. 1. 1.] \n",
      " [1. 1. 1. 1.] \n",
      " [0. 0. 0. 0.]\n"
     ]
    }
   ],
   "source": [
    "y = np.ones(4) * 2\n",
    "y.attach_grad()\n",
    "with autograd.record():\n",
    "    u = x * y\n",
    "    u.attach_grad()  # implicitly run u = u.detach()\n",
    "    z = u + x\n",
    "z.backward()\n",
    "print(x.grad, '\\n', u.grad, '\\n', y.grad)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Head gradients\n",
    "\n",
    "Detaching allows to breaks the computation into several parts. We could use chain rule :numref:`chapter_math` to compute the gradient for the whole computation.  Assume $u = f(x)$ and $z = g(u)$, by chain rule we have $\\frac{dz}{dx} = \\frac{dz}{du} \\frac{du}{dx}.$ To compute $\\frac{dz}{du}$, we can first detach $u$ from the computation and then call `z.backward()` to compute the first term."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1. 1. 1. 1.] \n",
      " [0. 0. 0. 0.]\n"
     ]
    }
   ],
   "source": [
    "y = np.ones(4) * 2\n",
    "y.attach_grad()\n",
    "with autograd.record():\n",
    "    u = x * y\n",
    "    v = u.detach()  # u still keeps the computation graph\n",
    "    v.attach_grad()\n",
    "    z = v + x\n",
    "z.backward()\n",
    "print(x.grad, '\\n', y.grad)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Subsequently, we can call `u.backward()` to compute the second term, \n",
    "but pass the first term as the head gradients to multiply both terms \n",
    "so that `x.grad` will contains $\\frac{dz}{dx}$ instead of $\\frac{du}{dx}$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2. 2. 2. 2.] \n",
      " [0. 1. 2. 3.]\n"
     ]
    }
   ],
   "source": [
    "u.backward(v.grad)\n",
    "print(x.grad, '\\n', y.grad)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Computing the Gradient of Python Control Flow\n",
    "\n",
    "One benefit of using automatic differentiation \n",
    "is that even if building the computational graph of a function \n",
    "required passing through a maze of Python control flow \n",
    "(e.g. conditionals, loops, and arbitrary function calls), \n",
    "we can still calculate the gradient of the resulting variable. \n",
    "In the following snippet, note that \n",
    "the number of iterations of the `while` loop \n",
    "and the evaluation of the `if` statement\n",
    "both depend on the value of the input `b`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "attributes": {
     "classes": [],
     "id": "",
     "n": "8"
    }
   },
   "outputs": [],
   "source": [
    "def f(a):\n",
    "    b = a * 2\n",
    "    while np.abs(b).sum() < 1000:\n",
    "        b = b * 2\n",
    "    if b.sum() > 0:\n",
    "        c = b\n",
    "    else:\n",
    "        c = 100 * b\n",
    "    return c"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Again to compute gradients, we just need to `record` the calculation\n",
    "and then call the `backward` function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "attributes": {
     "classes": [],
     "id": "",
     "n": "9"
    }
   },
   "outputs": [],
   "source": [
    "a = np.random.normal()\n",
    "a.attach_grad()\n",
    "with autograd.record():\n",
    "    d = f(a)\n",
    "d.backward()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can now analyze the `f` function defined above. \n",
    "Note that it is piecewise linear in its input `a`. \n",
    "In other words, for any `a` there exists some constant \n",
    "such that for a given range `f(a) = g * a`. \n",
    "Consequently `d / a` allows us to verify that the gradient is correct:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "attributes": {
     "classes": [],
     "id": "",
     "n": "10"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.0\n"
     ]
    }
   ],
   "source": [
    "print(a.grad == (d / a))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training Mode and Prediction Mode\n",
    "\n",
    "As we have seen, after we call `autograd.record`, \n",
    "MXNet logs the operations in the following block. \n",
    "There is one more subtle detail to be aware of.\n",
    "Additionally, `autograd.record` will change \n",
    "the running mode from *prediction* mode to *training* mode. \n",
    "We can verify this behavior by calling the `is_training` function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "attributes": {
     "classes": [],
     "id": "",
     "n": "7"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "False\n",
      "True\n"
     ]
    }
   ],
   "source": [
    "print(autograd.is_training())\n",
    "with autograd.record():\n",
    "    print(autograd.is_training())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When we get to complicated deep learning models,\n",
    "we will encounter some algorithms where the model\n",
    "behaves differently during training and \n",
    "when we subsequently use it to make predictions. \n",
    "The popular neural network techniques *dropout* :numref:`chapter_dropout` \n",
    "and *batch normalization* :numref:`chapter_batch_norm`\n",
    "both exhibit this characteristic.\n",
    "In other cases, our models may store auxiliary variables in *training* mode \n",
    "for purposes of make computing gradients easier \n",
    "that are not necessary at prediction time. \n",
    "We will cover these differences in detail in later chapters. \n",
    "\n",
    "\n",
    "## Summary\n",
    "\n",
    "* MXNet provides an `autograd` package to automate the calculation of derivatives. To use it, we first attach gradients to those variables with respect to which we desire partial derivartives. We then record the computation of our target value, executed its backward function, and access the resulting gradient via our variable's `grad` attribute.\n",
    "* We can detach gradients and pass head gradients to the backward function to control the part of the computation will be used in the backward function.\n",
    "* The running modes of MXNet include *training mode* and *prediction mode*. We can determine the running mode by calling `autograd.is_training()`.\n",
    "\n",
    "## Exercises\n",
    "\n",
    "1. Try to run `y.backward()` twice.\n",
    "1. In the control flow example where we calculate the derivative of `d` with respect to `a`, what would happen if we changed the variable `a` to a random vector or matrix. At this point, the result of the calculation `f(a)` is no longer a scalar. What happens to the result? How do we analyze this?\n",
    "1. Redesign an example of finding the gradient of the control flow. Run and analyze the result.\n",
    "1. In a second-price auction (such as in eBay or in computational advertising), the winning bidder pays the second-highest price. Compute the gradient of the final price with respect to the winning bidder's bid using `autograd`. What does the result tell you about the mechanism? If you are curious to learn more about second-price auctions, check out this paper by [Edelman, Ostrovski and Schwartz, 2005](https://www.benedelman.org/publications/gsp-060801.pdf).\n",
    "1. Why is the second derivative much more expensive to compute than the first derivative?\n",
    "1. Derive the head gradient relationship for the chain rule. If you get stuck, use the [\"Chain rule\" article on Wikipedia](https://en.wikipedia.org/wiki/Chain_rule).\n",
    "1. Assume $f(x) = \\sin(x)$. Plot $f(x)$ and $\\frac{df(x)}{dx}$ on a graph, where you computed the latter without any symbolic calculations, i.e. without exploiting that $f'(x) = \\cos(x)$.\n",
    "\n",
    "## Scan the QR Code to [Discuss](https://discuss.mxnet.io/t/2318)\n",
    "\n",
    "![](../img/qr_autograd.svg)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
